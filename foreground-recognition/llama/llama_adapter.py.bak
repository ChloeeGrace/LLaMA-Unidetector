import os
import json
from pathlib import Path

import clip
import torch
import torch.nn as nn
from timm.models.vision_transformer import Block

from .llama import ModelArgs, Transformer, BERTTransformer,BERTTransformer_cross
from .tokenizer import Tokenizer
from .utils import sample_top_p, _download

from .GroundingDINO.groundingdino.models import build_model
from .GroundingDINO.groundingdino.util import box_ops,get_tokenlizer
from .GroundingDINO.groundingdino.util.slconfig import SLConfig
from .GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap
from .GroundingDINO.groundingdino.util.vl_utils import create_positive_map_from_span

from transformers import BertTokenizer, BertModel


def load_model(model_config_path, model_checkpoint_path, cpu_only=False):
    args = SLConfig.fromfile(model_config_path)
    args.device = "cuda" if not cpu_only else "cpu"
    model = build_model(args)
    checkpoint = torch.load(model_checkpoint_path, map_location="cpu")
    load_res = model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)
    # print(load_res)
    # _ = model.eval()
    model.eval()
    return model


def to_template(input_dict):
    batch_sentence = []
    box_info_all = []
    for idx in range(len(input_dict)):
        box_info = input_dict[idx]["box"]
        phrase_info = input_dict[idx]["pred_phrases"]
        detect_info = []
        for box_id in range(len(box_info)):
            info = box_info[box_id]
            pred_cls = phrase_info[box_id].split("(")[0]
            detect_info.append(
                (pred_cls, str("%.3f"%(info[0].item())), str("%.3f"%(info[1].item())), str("%.3f"%(info[2].item())), str("%.3f"%(info[3].item()))))

            box_info_all.append((phrase_info[box_id],str("%.3f"%(info[0].item())), str("%.3f"%(info[1].item())), str("%.3f"%(info[2].item())), str("%.3f"%(info[3].item()))))
        sentence = "The bounding boxes are %s" % (detect_info)
        batch_sentence.append(sentence)
    return box_info_all, batch_sentence

class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight

class LLaMA_adapter(nn.Module):

    def __init__(self, llama_ckpt_dir, llama_tokenizer,
                 max_seq_len=512, max_batch_size=1,
                 clip_model='ViT-L/14@336px',
                 v_embed_dim=1024, v_depth=16,
                 v_num_heads=16, v_mlp_ratio=4.0,
                 query_len=577, query_layer=32, phase="finetune"):
        super().__init__()
        # llama configs
        with open(os.path.join(llama_ckpt_dir, "params.json"), "r") as f:
            params = json.loads(f.read())
        bias_lora = phase == "finetune"
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params
        ) # max_batch_size only affects inferenc

        # 1.1 clip and clip projector
        self.clip, self.clip_transform = clip.load(clip_model)

        clip_dim = self.clip.visual.proj.shape[1]
        self.clip_proj = nn.Linear(clip_dim, v_embed_dim)
        self.clip_proj_norm = nn.LayerNorm(v_embed_dim)

        self.query_len = query_len
        self.query_layer = query_layer

        # 1.2 grounding dino feature
        self.groundingdino = load_model("/data/wgq/DINO-LLAMA/LLAMA-Adapter-v3/llama/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py", "/data/wgq/DINO-LLAMA/LLAMA-Adapter-v3/llama/GroundingDINO/ckpt/groundingdino_swint_ogc.pth")
        self.dino_proj = nn.Linear(256, v_embed_dim) #update
        self.dino_proj_norm = nn.LayerNorm(v_embed_dim) #update

        self.pre_proj = nn.Linear(2362,577) #update
        self.pre_proj_norm = nn.LayerNorm(577) #update

        self.align_proj = nn.Linear(v_embed_dim*2,v_embed_dim) #update
        self.align_proj_norm = nn.LayerNorm(v_embed_dim) #update

        # 2. visual query, blocks and projector

        visual_model_args = ModelArgs(dim=1024, n_layers=16, n_heads=8, max_seq_len=577)
        visual_model_args.vocab_size = 1024
        self.visual_blocks = BERTTransformer(visual_model_args)
        self.visual_blocks_dino = BERTTransformer_cross(visual_model_args) #update


        self.visual_proj = nn.Linear(v_embed_dim, model_args.dim) #update
        self.visual_proj_norm = nn.LayerNorm(model_args.dim) #update

        # 3.1 perception query
        self.caption = "pedestrian . person . bicycle . car . motorcycle . airplane . bus . train . truck . boat . traffic light . fire hydrant . stop sign . parking meter . bench . bird . cat . dog . horse . sheep . cow . elephant . bear . zebra . giraffe . backpack . umbrella . handbag . tie . suitcase . frisbee . skis . snowboard . sports ball . kite . baseball bat . baseball glove . skateboard . surfboard . tennis racket . bottle . wine glass . cup . fork . knife . spoon . bowl . banana . apple . sandwich . orange . broccoli . carrot . hot dog . pizza . donut . cake . chair . couch . potted plant . bed . dining table . toilet . tv . laptop . mouse . remote . keyboard . cell phone . microwave . oven . toaster . sink . refrigerator . book . clock . vase . scissors . teddy bear . hair drier . toothbrush ."
        self.caption = self.caption.lower()
        self.caption = self.caption.strip()
        if not self.caption.endswith("."):
            self.caption = self.caption + "."


        self.box_threshold = 0.5
        self.text_threshold = 0.5

        self.groundingdino_tokenizer = BertTokenizer.from_pretrained("/data/wgq/bert-base-uncased")
        self.groundingdino_bert = BertModel.from_pretrained("/data/wgq/bert-base-uncased")
        self.groundingdino_max_len = 4096
        self.groundingdino_norm = nn.LayerNorm(self.groundingdino_max_len)

        # 3.2 adapter query
        self.adapter_query = nn.Embedding(
            query_len * query_layer, model_args.dim)

        # 4. tokenizer
        self.tokenizer = Tokenizer(model_path=llama_tokenizer)

        # 5. llama 
        model_args.w_bias = bias_lora
        model_args.w_lora = bias_lora
        model_args.vocab_size = self.tokenizer.n_words
        torch.set_default_tensor_type(torch.cuda.HalfTensor)
        self.llama = Transformer(model_args)
        torch.set_default_tensor_type(torch.FloatTensor)

        ckpts = sorted(Path(llama_ckpt_dir).glob("*.pth"))
        for ckpt in ckpts:
            print('load_ckpt_path:', ckpt)
            ckpt = torch.load(ckpt, map_location='cpu')
            self.llama.load_state_dict(ckpt, strict=False)

        for name, param in self.named_parameters():
            param.requires_grad = False
            if "dino_proj" in name:
                param.data = param.data.float()
                param.requires_grad = True

            if "pre_proj" in name:
                param.data = param.data.float()
                param.requires_grad = True

            if "align_proj" in name:
                param.data = param.data.float()
                param.requires_grad = True

            if "groundingdino_norm" in name:
                param.data = param.data.float()
                param.requires_grad = True

            if "visual_blocks_dino" in name:
                param.data = param.data.float()
                param.requires_grad = True


        for name, para in self.llama.named_parameters():
            if 'norm' in name:
                para.data = para.data.float()
                para.requires_grad = True
            if 'bias' in name:
                para.data = para.data.float()
                para.requires_grad = True
            if 'lora' in name:
                para.data = para.data.float()
                para.requires_grad = True

        # for name, param in self.named_parameters():
        #     if param.requires_grad:
        #        print(f"Trainable param: {name}, {param.shape}, {param.dtype}")

        # 6. training criterion
        self.criterion = torch.nn.CrossEntropyLoss(ignore_index=0)

    def clip_encode_image(self, x):
        # modified from CLIP
        x = self.clip.visual.conv1(x)  # shape = [*, width, grid, grid]
        # shape = [*, width, grid ** 2]
        x = x.reshape(x.shape[0], x.shape[1], -1)
        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]
        x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1,
                      x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]
        x = x + self.clip.visual.positional_embedding.to(x.dtype)
        x = self.clip.visual.ln_pre(x)

        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.clip.visual.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD

        # preserve all spatial tokens
        x = self.clip.visual.ln_post(x[:, :, :])

        if self.clip.visual.proj is not None:
            x = x @ self.clip.visual.proj

        return x

    def forward_visual(self, imgs, dino_proj):
        bs = imgs.shape[0]
        clip_feats = self.clip_encode_image(imgs)
        clip_feats = self.clip_proj_norm(self.clip_proj(clip_feats.float()))
        visual_query_clip = clip_feats

        visual_query_dino = self.dino_proj(dino_proj)
        visual_query_dino = self.dino_proj_norm(visual_query_dino)
        visual_query_dino = self.pre_proj_norm(self.pre_proj(visual_query_dino.transpose(1,2))).transpose(1,2)

        visual_query_clip, shared_query = self.visual_blocks(visual_query_clip, 0)
        visual_query_dino = self.visual_blocks_dino(shared_query,visual_query_dino,0)
        visual_query = self.align_proj_norm(self.align_proj(torch.cat([visual_query_clip,visual_query_dino],dim=2)))
        visual_query = self.visual_proj(visual_query)
        visual_query = self.visual_proj_norm(visual_query)

        return visual_query

    def forward_perception(self, imgs, bs, outputs):
        result_dict = {}
        for idx in range(bs):
            result_dict[idx] = {}
            logits = outputs["pred_logits"].sigmoid()[idx]
            boxes = outputs["pred_boxes"][idx]

            logits_filt = logits.cpu().clone()
            boxes_filt = boxes.cpu().clone()
            filt_mask = logits_filt.max(dim=1)[0] > self.box_threshold
            logits_filt = logits_filt[filt_mask]  # num_filt, 256
            boxes_filt = boxes_filt[filt_mask]  # num_filt, 4

            # get phrase
            tokenlizer = self.groundingdino.tokenizer
            tokenized = tokenlizer(self.caption)

            # build pred
            pred_phrases = []
            for logit, box in zip(logits_filt, boxes_filt):
                pred_phrase = get_phrases_from_posmap(logit > self.text_threshold, tokenized, tokenlizer)
                pred_phrases.append(pred_phrase + f"({str(logit.max().item())[:4]})")

            result_dict[idx]["box"] = boxes_filt
            result_dict[idx]["pred_phrases"] = pred_phrases

        #-----------------template---------------------------------------
        box_info, batch_sentences = to_template(result_dict)
        input_tokens = [self.groundingdino_tokenizer.encode(text,add_special_tokens=True) for text in batch_sentences]
        input_ids = [tokens + [self.groundingdino_tokenizer.pad_token_id]*(self.groundingdino_max_len - len(tokens)) for tokens in input_tokens]
        embedding_tensors = torch.FloatTensor(input_ids).to(imgs.device)
        embedding_tensors = self.groundingdino_norm(embedding_tensors)
        projected_tensor = embedding_tensors.view(bs,1,self.groundingdino_max_len)
        projected_tensor = projected_tensor.repeat(1,577,1)

        return box_info, projected_tensor

    def forward(self, tokens, labels, imgs, imgs_dino):
        #------------------grounding DINO-------------------------------------
        bs = imgs.shape[0]
        input_captions = [self.caption] * bs
        Dino_proj, outputs = self.groundingdino(imgs_dino, captions=input_captions)
        #---------------------------------------------------------------------
        visual_proj = self.forward_visual(imgs,Dino_proj)
        #--------------------------------perception output---------------------
        _, perception_proj = self.forward_perception(imgs_dino,bs,outputs)
        #----------------------------------------------------------------------
        _bsz, seqlen = tokens.shape

        h = self.llama.tok_embeddings(tokens)
        freqs_cis = self.llama.freqs_cis.to(h.device)
        freqs_cis = freqs_cis[:seqlen]
        mask = None
        mask = torch.full((1, 1, seqlen, seqlen), float("-inf"), device=h.device)
        mask = torch.triu(mask, diagonal=0 + 1).type_as(h)

        adapter = self.adapter_query.weight.reshape(self.query_layer, self.query_len, -1).unsqueeze(1)
        adapter_index = 0
        for layer in self.llama.layers:
            h = layer(h, 0, freqs_cis, mask, visual_proj + perception_proj + adapter[adapter_index])
            adapter_index = adapter_index + 1

        h = self.llama.norm(h)
        output = self.llama.output(h)
        output = output[:, :-1, :]
        labels = labels[:, 1:]

        if labels.sum() == 0:
            c_loss = output.mean() * 0
        else:
            assert self.llama.vocab_size == 32000
            c_loss = self.criterion(output.reshape(-1, self.llama.vocab_size), labels.flatten())

        return c_loss, c_loss

    @torch.inference_mode()
    def forward_inference(self, visual_proj, perception_proj, tokens, start_pos: int):
        _bsz, seqlen = tokens.shape
        h = self.llama.tok_embeddings(tokens)
        freqs_cis = self.llama.freqs_cis.to(h.device)
        freqs_cis = freqs_cis[start_pos : start_pos + seqlen]
        mask = None
        mask = torch.full((1, 1, seqlen, seqlen), float("-inf"), device=h.device)
        mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)


        adapter = self.adapter_query.weight.reshape(self.query_layer, self.query_len, -1).unsqueeze(1)
        adapter_index = 0

        for layer in self.llama.layers:
            h = layer(h, start_pos, freqs_cis, mask, visual_proj + perception_proj + adapter[adapter_index].repeat(_bsz, 1, 1))
            adapter_index = adapter_index + 1

        h = self.llama.norm(h)
        output = self.llama.output(h[:, -1, :])

        return output.float()

    @torch.inference_mode()
    def generate(
        self, imgs, imgs_dino, prompts,
        max_gen_len: int = 256,
        temperature: float = 0.0,
        top_p: float = 0.75,
    ):
        bsz = len(imgs)
        params = self.llama.params
        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)
        assert len(imgs) == len(prompts)

        #------------------grounding DINO-------------------------------------
        input_captions = [self.caption] * bsz
        Dino_proj, outputs = self.groundingdino(imgs_dino, captions=input_captions)
        #---------------------------------------------------------------------

        with torch.cuda.amp.autocast():
            visual_query = self.forward_visual(imgs,Dino_proj)
            batched_sentences, perception_query = self.forward_perception(imgs_dino,bsz,outputs)

        if isinstance(prompts[0], str):
            prompts = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]

        min_prompt_size = min([len(t) for t in prompts])
        max_prompt_size = max([len(t) for t in prompts])


        total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)

        tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()

        for k, t in enumerate(prompts):
            tokens[k, : len(t)] = torch.tensor(t).cuda().long()
        input_text_mask = tokens != self.tokenizer.pad_id
        start_pos = min_prompt_size
        prev_pos = 0

        conf_list = [] #---conf
        s_flag = False
        for cur_pos in range(start_pos, total_len):
            with torch.cuda.amp.autocast():
                logits = self.forward_inference(visual_query, perception_query, tokens[:, prev_pos:cur_pos], prev_pos)
            if temperature > 0:
                probs = torch.softmax(logits / temperature, dim=-1)
                next_token = sample_top_p(probs, top_p)
            else:
                next_token = torch.argmax(logits, dim=-1)
                
                #----------------------------- calculate conf---------------------------
                if prev_pos == 0:
                    pass
                else:
                    
                    check_id = tokens[:,prev_pos:cur_pos].tolist()[0]
                    decode_check_id = self.tokenizer.decode(check_id)
                #--------------------------------------------------------check next----------------
                    check_next_token = next_token.reshape(-1)

                    check_next_token_id = torch.where(
                        input_text_mask[:, cur_pos], tokens[:, cur_pos], check_next_token
                    ).tolist()[0]

                    decode_next_token = self.tokenizer.decode(check_next_token_id)

                #--------------------------------------------------------------------------------

                    if '[' in decode_next_token:
                        conf = 0
                        count = 0
                        s_flag = True

                    else:
                        if s_flag == False:
                            pass
                        elif ']' not in decode_next_token and s_flag == True: #']'

                            if decode_next_token == "'" or decode_next_token == "," or decode_next_token == "'," or decode_next_token == "" or decode_next_token == ", " or decode_next_token == "', " or decode_next_token == ".":
                                pass
                            elif  decode_check_id == ".":
                                pass 
                            else:
                                # print(decode_next_token, torch.max(torch.softmax(logits, dim=-1)).tolist())
                                conf += torch.max(torch.softmax(logits, dim=-1)).tolist()
                                count += 1
                        else:
                            conf = conf / count
                            conf_list.append(conf)
                #-------------------------------------------------------------------------
            next_token = next_token.reshape(-1)

            next_token = torch.where(
                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token
            )
            tokens[:, cur_pos] = next_token
            # trick: early stop if bsz==1
            if bsz == 1 and next_token[0] == self.tokenizer.eos_id:
                break
            prev_pos = cur_pos

        decoded = []
        for i, t in enumerate(tokens.tolist()):

            # cut to max gen len
            t = t[len(prompts[i]): len(prompts[i]) + max_gen_len]
            # cut to eos tok if any
            try:
                t = t[: t.index(self.tokenizer.eos_id)]
            except ValueError:
                pass
            decoded.append(self.tokenizer.decode(t))
        
        #--------------- add conf ---------------------
        # import pdb 
        # pdb.set_trace()

        if temperature == 99999:
            result = decoded[0]
            det_num = len(conf_list)
            insert_list = []
            flag = False
            end_idx = 0
            for s_i in range(len(result) - 1):
                sub_str = result[s_i]
                next_sub_str = result[s_i + 1]
                if sub_str == "[" and next_sub_str != "[":
                    flag = True
                else:
                    
                    if sub_str == "]":
                        end_idx = s_i + 1

                    if sub_str == "," and flag == True:
                        if det_num > 0:
                            det_num -= 1
                        else:
                            break
                        insert_list.append(s_i+1)
                        flag = False
                        
                    else:
                        continue
            result = result[:end_idx]
            result_out = []

            for i in range(len(insert_list)):
                insert_id = insert_list[i]
                if i == 0:
                    start_id = 0
                else:
                    start_id = insert_list[i-1]

                sub_result = result[start_id:insert_id] + " %.2f,"%(conf_list[i])

                if i == len(insert_list) - 1:
                    sub_result = sub_result + result[insert_id:] + "]."
                result_out.append(sub_result)

            result = ""
            for i in range(len(result_out)):
                result += result_out[i]

            decoded[0] = result
        return batched_sentences, decoded


_MODELS = {
    "BIAS-7B": "https://github.com/ZrrSkywalker/LLaMA-Adapter/releases/download/v.2.0.0/7fa55208379faf2dd862565284101b0e4a2a72114d6490a95e432cf9d9b6c813_BIAS-7B.pth",
    "BIAS-7B_chinese": "",
    # "LORA16-7B": "",
    # "PARTIAL-7B": ""
}

def available_models():
    return list(_MODELS.keys())

def load(name, llama_dir, device="cuda" if torch.cuda.is_available() else "cpu", download_root='ckpts', max_seq_len=512,
        phase="finetune"):
    if name in _MODELS:
        model_path = _download(_MODELS[name], download_root)
    elif os.path.isfile(name):
        model_path = name
    else:
        return RuntimeError(f"Model {name} not found; available models = {available_models()}")

    ckpt = torch.load(model_path, map_location='cpu')
    
    # BIAS-7B or https://xxx/sha256_BIAS-7B.pth -> 7B
    llama_type = name.split('.')[0].split('-')[-1]
    llama_ckpt_dir = os.path.join(llama_dir, llama_type)
    print("llama_type:%s"%(llama_type))
    llama_tokenzier_path = os.path.join(llama_dir, 'tokenizer.model')

    # load llama_adapter weights and model_cfg
    print(f'Loading LLaMA-Adapter from {model_path}')
    ckpt = torch.load(model_path, map_location='cpu')

    model = LLaMA_adapter(
        llama_ckpt_dir, llama_tokenzier_path,
        max_seq_len=max_seq_len, max_batch_size=32,
        clip_model='ViT-L/14@336px',
        v_embed_dim=1024, v_depth=16,
        v_num_heads=16, v_mlp_ratio=4.0,
        query_len=577, query_layer=32,
        phase=phase)

    load_result = model.load_state_dict(ckpt['model'], strict=False)

    assert len(load_result.unexpected_keys) == 0, f"Unexpected keys: {load_result.unexpected_keys}"
    return model.to(device), model.clip_transform